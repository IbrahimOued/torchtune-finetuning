{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtune.models.llama2 import llama2_7b, lora_llama2_7b\n",
    "from torchtune.models.llama2 import llama2_tokenizer\n",
    "from torchtune.modules.peft.peft_utils import get_adapter_params, set_trainable_params\n",
    "from torchtune.datasets import text_completion_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and dataset preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring files matching the following patterns: *.safetensors\n",
      "Fetching 15 files: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 15/15 [00:00<00:00, 30870.74it/s]\n",
      "Successfully downloaded model repo and wrote to the following locations:\n",
      "/tmp/Llama-2-7b-hf/.cache\n",
      "/tmp/Llama-2-7b-hf/LICENSE.txt\n",
      "/tmp/Llama-2-7b-hf/config.json\n",
      "/tmp/Llama-2-7b-hf/README.md\n",
      "/tmp/Llama-2-7b-hf/generation_config.json\n",
      "/tmp/Llama-2-7b-hf/USE_POLICY.md\n",
      "/tmp/Llama-2-7b-hf/.gitattributes\n",
      "/tmp/Llama-2-7b-hf/model.safetensors.index.json\n",
      "/tmp/Llama-2-7b-hf/special_tokens_map.json\n",
      "/tmp/Llama-2-7b-hf/tokenizer_config.json\n",
      "/tmp/Llama-2-7b-hf/tokenizer.json\n",
      "/tmp/Llama-2-7b-hf/pytorch_model.bin.index.json\n",
      "/tmp/Llama-2-7b-hf/tokenizer.model\n",
      "/tmp/Llama-2-7b-hf/Responsible-Use-Guide.pdf\n",
      "/tmp/Llama-2-7b-hf/pytorch_model-00002-of-00002.bin\n",
      "/tmp/Llama-2-7b-hf/pytorch_model-00001-of-00002.bin\n",
      "/tmp/Llama-2-7b-hf/torchtune_config.yaml\n"
     ]
    }
   ],
   "source": [
    "!tune download meta-llama/Llama-2-7b-hf \\\n",
    "  --output-dir /tmp/Llama-2-7b-hf \\\n",
    "  --hf-token $HF_TOKEN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creation of the base and LoRA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = llama2_7b()\n",
    "lora_model = lora_llama2_7b(lora_attn_modules=['q_proj','k_proj','v_proj','output_proj'], lora_rank=32, lora_alpha=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing the base model and the LoRA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Base model ===\n",
      "CausalSelfAttention(\n",
      "  (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (output_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "  (pos_embeddings): RotaryPositionalEmbeddings()\n",
      ")\n",
      "=== LoRA model ===\n",
      "CausalSelfAttention(\n",
      "  (q_proj): LoRALinear(\n",
      "    (dropout): Dropout(p=0.05, inplace=False)\n",
      "    (lora_a): Linear(in_features=4096, out_features=32, bias=False)\n",
      "    (lora_b): Linear(in_features=32, out_features=4096, bias=False)\n",
      "  )\n",
      "  (k_proj): LoRALinear(\n",
      "    (dropout): Dropout(p=0.05, inplace=False)\n",
      "    (lora_a): Linear(in_features=4096, out_features=32, bias=False)\n",
      "    (lora_b): Linear(in_features=32, out_features=4096, bias=False)\n",
      "  )\n",
      "  (v_proj): LoRALinear(\n",
      "    (dropout): Dropout(p=0.05, inplace=False)\n",
      "    (lora_a): Linear(in_features=4096, out_features=32, bias=False)\n",
      "    (lora_b): Linear(in_features=32, out_features=4096, bias=False)\n",
      "  )\n",
      "  (output_proj): LoRALinear(\n",
      "    (dropout): Dropout(p=0.05, inplace=False)\n",
      "    (lora_a): Linear(in_features=4096, out_features=32, bias=False)\n",
      "    (lora_b): Linear(in_features=32, out_features=4096, bias=False)\n",
      "  )\n",
      "  (pos_embeddings): RotaryPositionalEmbeddings()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Base model ===\")\n",
    "print(base_model.layers[0].attn)\n",
    "print(\"=== LoRA model ===\")\n",
    "print(lora_model.layers[0].attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing the number of total and trainable parameters in the LoRA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  6771970048 total params,\n",
      "  33554432 trainable params,\n",
      "  0.50% of all params are trainable.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# Once weâ€™ve loaded the base model weights, we also want to set only LoRA parameters to trainable.\n",
    "\n",
    "# Fetch all params from the model that are associated with LoRA.\n",
    "lora_params = get_adapter_params(lora_model)\n",
    "# Set requires_grad=True on lora_params, and requires_grad=False on all others.\n",
    "set_trainable_params(lora_model, lora_params)\n",
    "# Print the total number of parameters\n",
    "total_params = sum([p.numel() for p in lora_model.parameters()])\n",
    "trainable_params = sum([p.numel() for p in lora_model.parameters() if p.requires_grad])\n",
    "print(\n",
    "  f\"\"\"\n",
    "  {total_params} total params,\n",
    "  {trainable_params} trainable params,\n",
    "  {(100.0 * trainable_params / total_params):.2f}% of all params are trainable.\n",
    "  \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model with the LoRA model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy the recipe and config files in current dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists at /u/ouedraoi/documents/projects/torchtune-finetuning/recipes/lora_finetune_distributed.py, not overwriting.\n",
      "File already exists at /u/ouedraoi/documents/projects/torchtune-finetuning/configs/custom_lora.yaml, not overwriting.\n"
     ]
    }
   ],
   "source": [
    "!tune cp -n lora_finetune_distributed ./recipes/lora_finetune_distributed.py --make-parents\n",
    "!tune cp -n llama2/7B_lora ./configs/custom_lora.yaml --make-parents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running with torchrun...\n",
      "W0826 12:40:32.823000 140686333332352 torch/distributed/run.py:779] \n",
      "W0826 12:40:32.823000 140686333332352 torch/distributed/run.py:779] *****************************************\n",
      "W0826 12:40:32.823000 140686333332352 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n",
      "W0826 12:40:32.823000 140686333332352 torch/distributed/run.py:779] *****************************************\n",
      "INFO:torchtune.utils.logging:Running LoRAFinetuneRecipeDistributed with resolved config:\n",
      "\n",
      "batch_size: 2\n",
      "checkpointer:\n",
      "  _component_: torchtune.utils.FullModelHFCheckpointer\n",
      "  adapter_checkpoint: null\n",
      "  checkpoint_dir: /tmp/Llama-2-7b-hf\n",
      "  checkpoint_files:\n",
      "  - pytorch_model-00001-of-00002.bin\n",
      "  - pytorch_model-00002-of-00002.bin\n",
      "  model_type: LLAMA2\n",
      "  output_dir: /tmp/Llama-2-7b-hf\n",
      "  recipe_checkpoint: null\n",
      "dataset:\n",
      "  _component_: torchtune.datasets.alpaca_dataset\n",
      "device: cuda\n",
      "dtype: bf16\n",
      "enable_activation_checkpointing: false\n",
      "epochs: 10\n",
      "gradient_accumulation_steps: 32\n",
      "log_every_n_steps: 5\n",
      "log_peak_memory_stats: false\n",
      "loss:\n",
      "  _component_: torch.nn.CrossEntropyLoss\n",
      "lr_scheduler:\n",
      "  _component_: torchtune.modules.get_cosine_schedule_with_warmup\n",
      "  num_warmup_steps: 100\n",
      "max_steps_per_epoch: null\n",
      "metric_logger:\n",
      "  _component_: torchtune.utils.metric_logging.WandBLogger\n",
      "  group: torchtune_experiments\n",
      "  job_type: lora_finetune_distributed_device\n",
      "  project: finetune-llama2-7B-hf\n",
      "model:\n",
      "  _component_: torchtune.models.llama2.lora_llama2_7b\n",
      "  apply_lora_to_mlp: false\n",
      "  apply_lora_to_output: false\n",
      "  lora_alpha: 64\n",
      "  lora_attn_modules:\n",
      "  - q_proj\n",
      "  - k_proj\n",
      "  - v_proj\n",
      "  - output_proj\n",
      "  lora_rank: 32\n",
      "optimizer:\n",
      "  _component_: torch.optim.AdamW\n",
      "  lr: 0.0003\n",
      "  weight_decay: 0.01\n",
      "output_dir: /tmp/lora_finetune_output\n",
      "resume_from_checkpoint: false\n",
      "seed: null\n",
      "shuffle: true\n",
      "tokenizer:\n",
      "  _component_: torchtune.models.llama2.llama2_tokenizer\n",
      "  path: /tmp/Llama-2-7b-hf/tokenizer.model\n",
      "\n",
      "DEBUG:torchtune.utils.logging:Setting manual seed to local seed 2672818458. Local seed is seed + rank = 2672818458 + 0\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mibrahim-oued\u001b[0m (\u001b[33mibrahim-oued-\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.17.7\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/u/ouedraoi/documents/projects/torchtune-finetuning/wandb/run-20240826_124043-l6mns6wy\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdauntless-disco-11\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ibrahim-oued-/finetune-llama2-7B-hf\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ibrahim-oued-/finetune-llama2-7B-hf/runs/l6mns6wy\u001b[0m\n",
      "INFO:torchtune.utils.logging:Logging /tmp/Llama-2-7b-hf/torchtune_config.yaml to W&B under Files\n",
      "INFO:torchtune.utils.logging:FSDP is enabled. Instantiating Model on CPU for Rank 0 ...\n",
      "INFO:torchtune.utils.logging:Model instantiation took 10.71 secs\n",
      "INFO:torchtune.utils.logging:Memory stats after model init:\n",
      "\tGPU peak memory allocation: 4.54 GB\n",
      "\tGPU peak memory reserved: 5.28 GB\n",
      "\tGPU peak memory active: 4.54 GB\n",
      "INFO:torchtune.utils.logging:Optimizer and loss are initialized.\n",
      "Downloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.47k/7.47k [00:00<00:00, 20.7kB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24.2M/24.2M [00:02<00:00, 10.4MB/s]\n",
      "Generating train split: 100%|â–ˆâ–ˆâ–ˆ| 52002/52002 [00:01<00:00, 44950.93 examples/s]\n",
      "INFO:torchtune.utils.logging:Dataset and Sampler are initialized.\n",
      "INFO:torchtune.utils.logging:Learning rate scheduler is initialized.\n",
      "WARNING:torchtune.utils.logging: Profiling disabled.\n",
      "INFO:torchtune.utils.logging: Profiler config after instantiation: {'enabled': False}\n",
      "1|192|Loss: 0.8369829058647156:  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 192/203 [4:39:53<16:05, 87.76s/it]"
     ]
    }
   ],
   "source": [
    "!tune run --nnodes 1 --nproc_per_node 4 ./recipes/lora_finetune_distributed.py --config ./configs/custom_lora.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy the evaluation config file to current dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists at /u/ouedraoi/documents/projects/torchtune-finetuning/configs/custom_eval_config.yaml, not overwriting.\n"
     ]
    }
   ],
   "source": [
    "!tune cp -n eleuther_evaluation ./configs/custom_eval_config.yaml --make-parents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tune run eleuther_eval --config ./configs/custom_eval_config.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
