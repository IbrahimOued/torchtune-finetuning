{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-End Workflow with torchtune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fine-tuning an LLM is usually only one step in a larger workflow. An example workflow that you might have can look something like this:\n",
    "\n",
    "Download a popular model from HF Hub\n",
    "\n",
    "Fine-tune the model using a relevant fine-tuning technique. The exact technique used will depend on factors such as the model, amount and nature of training data, your hardware setup and the end task for which the model will be used\n",
    "\n",
    "Evaluate the model on some benchmarks to validate model quality\n",
    "\n",
    "Run some generations to make sure the model output looks reasonable\n",
    "\n",
    "Quantize the model for efficient inference\n",
    "\n",
    "[Optional] Export the model for specific environments such as inference on a mobile phone\n",
    "\n",
    "In this tutorial, we’ll cover how you can use torchtune for all of the above, leveraging integrations with popular tools and libraries from the ecosystem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Gemma 2B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring files matching the following patterns: *.safetensors\n",
      "Fetching 9 files: 100%|████████████████████████| 9/9 [00:00<00:00, 14304.18it/s]\n",
      "Successfully downloaded model repo and wrote to the following locations:\n",
      "/tmp/google/gemma-2-2b/.cache\n",
      "/tmp/google/gemma-2-2b/README.md\n",
      "/tmp/google/gemma-2-2b/.gitattributes\n",
      "/tmp/google/gemma-2-2b/config.json\n",
      "/tmp/google/gemma-2-2b/model.safetensors.index.json\n",
      "/tmp/google/gemma-2-2b/special_tokens_map.json\n",
      "/tmp/google/gemma-2-2b/tokenizer_config.json\n",
      "/tmp/google/gemma-2-2b/generation_config.json\n",
      "/tmp/google/gemma-2-2b/tokenizer.model\n",
      "/tmp/google/gemma-2-2b/tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "!tune download google/gemma-2-2b \\\n",
    "  --output-dir /tmp/google/gemma-2-2b \\\n",
    "  --hf-token $HF_TOKEN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune the model using LoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial, we’ll fine-tune the model using LoRA. LoRA is a parameter efficient fine-tuning technique which is especially helpful when you don’t have a lot of GPU memory to play with. LoRA freezes the base LLM and adds a very small percentage of learnable parameters. This helps keep memory associated with gradients and optimizer state low. Using torchtune, you should be able to fine-tune a Llama2 7B model with LoRA in less than 16GB of GPU memory using bfloat16 on a RTX 3090/4090.\n",
    "\n",
    "We’ll fine-tune using our single device LoRA recipe and use the standard settings from the default config.\n",
    "\n",
    "This will fine-tune our model using a `batch_size=2` and `dtype=bfloat16`. With these settings the model should have a peak memory usage of ~16GB and total training time of around two hours for each epoch. We’ll need to make some changes to the config to make sure our recipe can access the right checkpoints.\n",
    "\n",
    "Let’s look for the right config for this use case by using the tune CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RECIPE                                   CONFIG                                  \n",
      "full_finetune_single_device              llama2/7B_full_low_memory               \n",
      "                                         code_llama2/7B_full_low_memory          \n",
      "                                         llama3/8B_full_single_device            \n",
      "                                         llama3_1/8B_full_single_device          \n",
      "                                         mistral/7B_full_low_memory              \n",
      "                                         phi3/mini_full_low_memory               \n",
      "full_finetune_distributed                llama2/7B_full                          \n",
      "                                         llama2/13B_full                         \n",
      "                                         llama3/8B_full                          \n",
      "                                         llama3_1/8B_full                        \n",
      "                                         llama3/70B_full                         \n",
      "                                         llama3_1/70B_full                       \n",
      "                                         mistral/7B_full                         \n",
      "                                         gemma/2B_full                           \n",
      "                                         gemma/7B_full                           \n",
      "                                         phi3/mini_full                          \n",
      "lora_finetune_single_device              llama2/7B_lora_single_device            \n",
      "                                         llama2/7B_qlora_single_device           \n",
      "                                         code_llama2/7B_lora_single_device       \n",
      "                                         code_llama2/7B_qlora_single_device      \n",
      "                                         llama3/8B_lora_single_device            \n",
      "                                         llama3_1/8B_lora_single_device          \n",
      "                                         llama3/8B_qlora_single_device           \n",
      "                                         llama3_1/8B_qlora_single_device         \n",
      "                                         llama2/13B_qlora_single_device          \n",
      "                                         mistral/7B_lora_single_device           \n",
      "                                         mistral/7B_qlora_single_device          \n",
      "                                         gemma/2B_lora_single_device             \n",
      "                                         gemma/2B_qlora_single_device            \n",
      "                                         gemma/7B_lora_single_device             \n",
      "                                         gemma/7B_qlora_single_device            \n",
      "                                         phi3/mini_lora_single_device            \n",
      "                                         phi3/mini_qlora_single_device           \n",
      "lora_dpo_single_device                   llama2/7B_lora_dpo_single_device        \n",
      "lora_dpo_distributed                     llama2/7B_lora_dpo                      \n",
      "lora_finetune_distributed                llama2/7B_lora                          \n",
      "                                         llama2/13B_lora                         \n",
      "                                         llama2/70B_lora                         \n",
      "                                         llama3/70B_lora                         \n",
      "                                         llama3_1/70B_lora                       \n",
      "                                         llama3/8B_lora                          \n",
      "                                         llama3_1/8B_lora                        \n",
      "                                         mistral/7B_lora                         \n",
      "                                         gemma/2B_lora                           \n",
      "                                         gemma/7B_lora                           \n",
      "                                         phi3/mini_lora                          \n",
      "lora_finetune_fsdp2                      llama2/7B_lora                          \n",
      "                                         llama2/13B_lora                         \n",
      "                                         llama2/70B_lora                         \n",
      "                                         llama2/7B_qlora                         \n",
      "                                         llama2/70B_qlora                        \n",
      "generate                                 generation                              \n",
      "eleuther_eval                            eleuther_evaluation                     \n",
      "quantize                                 quantization                            \n",
      "qat_distributed                          llama2/7B_qat_full                      \n",
      "                                         llama3/8B_qat_full                      \n"
     ]
    }
   ],
   "source": [
    "!tune ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial we’ll use the gemma/2B_lora_distributed config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tune run lora_finetune_distributed \\\n",
    "--config gemma/2B_lora \\\n",
    "checkpointer.checkpoint_dir=/tmp/google/gemma-2-2b \\\n",
    "tokenizer.path=/tmp/google/gemma-2-2b/tokenizer.model \\\n",
    "checkpointer.output_dir=/tmp/google/gemma-2-2b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final trained weights are merged with the original model and split across two checkpoint files similar to the source checkpoints from the HF Hub. In fact the keys will be identical between these checkpoints. We also have a third checkpoint file which is much smaller in size and contains the learnt LoRA adapter weights. For this tutorial, we’ll only use the model checkpoints and not the adapter weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Evaluation using EleutherAI’s Eval Harness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve fine-tuned a model. But how well does this model really do? Let’s run some Evaluations!\n",
    "\n",
    "torchtune integrates with [EleutherAI’s evaluation harness](https://github.com/EleutherAI/lm-evaluation-harness). An example of this is available through the `eleuther_eval` recipe. In this tutorial, we’re going to directly use this recipe by modifying its associated config `eleuther_evaluation.yaml`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this section of the tutorial, you should first run `pip install lm_eval==0.4.*` to install the EleutherAI evaluation harness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install lm_eval==0.4.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we plan to update all of the checkpoint files to point to our fine-tuned checkpoints, **let’s first copy over the config to our local working directory so we can make changes**. This will be easier than overriding all of these elements through the CLI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tune cp eleuther_evaluation ./custom_eval_config.yaml \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this tutorial we’ll use the `truthfulqa_mc2` (Use what has been given in the Deep NLP course) task from the harness. This task measures a model’s propensity to be truthful when answering questions and measures the model’s zero-shot accuracy on a question followed by one or more true responses and one or more false responses. Let’s first run a baseline without fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tune run eleuther_eval --config ./custom_eval_config.yaml \\\n",
    "checkpointer.checkpoint_dir=/tmp/google/gemma-2-2b \\\n",
    "tokenizer.path=/tmp/google/gemma-2-2b/tokenizer.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has an accuracy around 38.8%. Let’s compare this with the fine-tuned model.\n",
    "\n",
    "First, we modify custom_eval_config.yaml to include the fine-tuned checkpoints.\n",
    "\n",
    "```yaml\n",
    "checkpointer:\n",
    "    _component_: torchtune.utils.FullModelHFCheckpointer\n",
    "\n",
    "    # directory with the checkpoint files\n",
    "    # this should match the output_dir specified during\n",
    "    # finetuning\n",
    "    checkpoint_dir: <checkpoint_dir>\n",
    "\n",
    "    # checkpoint files for the fine-tuned model. This should\n",
    "    # match what's shown in the logs above\n",
    "    checkpoint_files: [\n",
    "        hf_model_0001_0.pt,\n",
    "        hf_model_0002_0.pt,\n",
    "    ]\n",
    "\n",
    "    output_dir: <checkpoint_dir>\n",
    "    model_type: GEMMA\n",
    "\n",
    "# Make sure to update the tokenizer path to the right\n",
    "# checkpoint directory as well\n",
    "tokenizer:\n",
    "    _component_: torchtune.models.llama2.llama2_tokenizer\n",
    "    path: <checkpoint_dir>/tokenizer.model\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s run the recipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tune run eleuther_eval --config ./custom_eval_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our fine-tuned model gets ~48% on this task, which is ~10 points better than the baseline. Great! Seems like our fine-tuning helped."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ve run some evaluations and the model seems to be doing well. But does it really generate meaningful text for the prompts you care about? Let’s find out!\n",
    "\n",
    "For this, we’ll use the [generate recipe](https://github.com/pytorch/torchtune/blob/main/recipes/generate.py) and the associated [config](https://github.com/pytorch/torchtune/blob/main/recipes/configs/generation.yaml).\n",
    "\n",
    "Let’s first copy over the config to our local working directory so we can make changes.\n",
    "\n",
    "Let’s modify custom_generation_config.yaml to include the following changes.\n",
    "\n",
    "```yaml\n",
    "checkpointer:\n",
    "    _component_: torchtune.utils.FullModelHFCheckpointer\n",
    "\n",
    "    # directory with the checkpoint files\n",
    "    # this should match the output_dir specified during\n",
    "    # finetuning\n",
    "    checkpoint_dir: <checkpoint_dir>\n",
    "\n",
    "    # checkpoint files for the fine-tuned model. This should\n",
    "    # match what's shown in the logs above\n",
    "    checkpoint_files: [\n",
    "        hf_model_0001_0.pt,\n",
    "        hf_model_0002_0.pt,\n",
    "    ]\n",
    "\n",
    "    output_dir: /tmp/google/gemma-2-2b\n",
    "    model_type: GEMMA\n",
    "\n",
    "# Make sure to update the tokenizer path to the right\n",
    "# checkpoint directory as well\n",
    "tokenizer:\n",
    "    _component_: torchtune.models.llama2.llama2_tokenizer\n",
    "    path: <checkpoint_dir>/tokenizer.model\n",
    "```\n",
    "\n",
    "Once the config is updated, let’s kick off generation! We’ll use the default settings for sampling with top_k=300 and a temperature=0.8. These parameters control how the probabilities for sampling are computed. These are standard settings for Llama2 7B and we recommend inspecting the model with these before playing around with these parameters.\n",
    "\n",
    "We’ll use a different prompt from the one in the config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tune run generate --config ./custom_generation_config.yaml \\\n",
    "prompt=\"What are some interesting sites to visit in the Bay Area?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, the bridge is pretty cool! Seems like our LLM knows a little something about the Bay Area!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Speeding up Generation using Quantization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that the generation recipe took around 11.6 seconds to generate 300 tokens. One technique commonly used to speed up inference is quantization. torchtune provides an integration with the [TorchAO](https://github.com/pytorch-labs/ao) quantization APIs. Let’s first quantize the model using 4-bit weights-only quantization and see if this improves generation speed.\n",
    "\n",
    "For this, we’ll use the [quantization recipe](https://github.com/pytorch/torchtune/blob/main/recipes/quantize.py).\n",
    "\n",
    "Let’s first copy over the config to our local working directory so we can make changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tune cp quantization ./custom_quantization_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s modify `custom_quantization_config.yaml` to include the following changes.\n",
    "\n",
    "```yml\n",
    "checkpointer:\n",
    "    _component_: torchtune.utils.FullModelHFCheckpointer\n",
    "\n",
    "    # directory with the checkpoint files\n",
    "    # this should match the output_dir specified during\n",
    "    # finetuning\n",
    "    checkpoint_dir: /tmp/google/gemma-2-2b\n",
    "\n",
    "    # checkpoint files for the fine-tuned model. This should\n",
    "    # match what's shown in the logs above\n",
    "    checkpoint_files: [\n",
    "        hf_model_0001_0.pt,\n",
    "        hf_model_0002_0.pt,\n",
    "    ]\n",
    "\n",
    "    output_dir: /tmp/google/gemma-2-2b\n",
    "    model_type: GEMMA\n",
    "```\n",
    "\n",
    "Once the config is updated, let’s kick off quantization! We’ll use the default quantization method from the config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tune run quantize --config ./custom_quantization_config.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Unlike the fine-tuned checkpoints, this outputs a single checkpoint file. This is because our quantization APIs currently don’t support any conversion across formats. As a result you won’t be able to use these quantized models outside of torchtune. But you should be able to use these with the generation and evaluation recipes within torchtune. These results will help inform which quantization methods you should use with your favorite inference engine.\n",
    "\n",
    "Now that we have the quantized model, let’s re-run generation.\n",
    "\n",
    "Modify custom_generation_config.yaml to include the following changes.\n",
    "\n",
    "```yml\n",
    "checkpointer:\n",
    "    # we need to use the custom torchtune checkpointer\n",
    "    # instead of the HF checkpointer for loading\n",
    "    # quantized models\n",
    "    _component_: torchtune.utils.FullModelTorchTuneCheckpointer\n",
    "\n",
    "    # directory with the checkpoint files\n",
    "    # this should match the output_dir specified during\n",
    "    # finetuning\n",
    "    checkpoint_dir: <checkpoint_dir>\n",
    "\n",
    "    # checkpoint files point to the quantized model\n",
    "    checkpoint_files: [\n",
    "        hf_model_0001_0-4w.pt,\n",
    "    ]\n",
    "\n",
    "    output_dir: <checkpoint_dir>\n",
    "    model_type: LLAMA2\n",
    "\n",
    "# we also need to update the quantizer to what was used during\n",
    "# quantization\n",
    "quantizer:\n",
    "    _component_: torchtune.utils.quantization.Int4WeightOnlyQuantizer\n",
    "    groupsize: 256\n",
    "```\n",
    "Once the config is updated, let’s kick off generation! We’ll use the same sampling parameters as before. We’ll also use the same prompt we did with the unquantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tune run generate --config ./custom_generation_config.yaml \\\n",
    "prompt=\"What are some interesting sites to visit in the Bay Area?\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
