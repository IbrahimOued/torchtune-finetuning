{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring dataset for Fine-tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Datasets are a core component of fine-tuning workflows that serve as a “steering wheel” to guide LLM generation for a particular use case. Many publicly shared open-source datasets have become popular for fine-tuning LLMs and serve as a great starting point to train your model. torchtune gives you the tools to download external community datasets, load in custom local datasets, or create your own datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Built-in datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use one of the built-in datasets in the library, simply import and call the dataset builder function. You can see a list of all supported datasets here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "from torchtune.datasets import alpaca_dataset\n",
    "\n",
    "# Load in tokenizer\n",
    "tokenizer = ...\n",
    "dataset = alpaca_dataset(tokenizer)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yaml\n",
    "# YAML config\n",
    "dataset:\n",
    "  _component_: torchtune.datasets.alpaca_dataset\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command line\n",
    "!tune run full_finetune_single_device --config llama3/8B_full_single_device \\\n",
    "dataset=torchtune.datasets.alpaca_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hugging Face datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide first class support for datasets on the Hugging Face hub. Under the hood, all of our built-in datasets and dataset builders are using Hugging Face’s `load_dataset()` to load in your data, whether local or on the hub.\n",
    "\n",
    "You can pass in a Hugging Face dataset path to the `source` parameter in any of our builders to specify which dataset on the hub to download. Additionally, all builders accept any keyword-arguments that `load_dataset()` supports. You can see a full list on Hugging Face’s documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtune.datasets import text_completion_dataset\n",
    "\n",
    "# Load in tokenizer\n",
    "tokenizer = ...\n",
    "dataset = text_completion_dataset(\n",
    "    tokenizer,\n",
    "    source=\"allenai/c4\",\n",
    "    # Keyword-arguments that are passed into load_dataset\n",
    "    split=\"train\",\n",
    "    data_dir=\"realnewslike\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```yml\n",
    "# YAML config\n",
    "dataset:\n",
    "  _component_: torchtune.datasets.text_completion_dataset\n",
    "  source: allenai/c4\n",
    "  split: train\n",
    "  data_dir: realnewslike\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Command line\n",
    "!tune run full_finetune_single_device --config llama3/8B_full_single_device \\\n",
    "dataset=torchtune.datasets.text_completion_dataset dataset.source=allenai/c4 \\\n",
    "dataset.split=train dataset.data_dir=realnewslike"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting max sequence length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default collator `padded_collate()` used in all our training recipes will pad samples to the max sequence length within the batch, not globally. If you wish to set an upper limit on the max sequence length globally, you can specify it in the dataset builder with max_seq_len. Any sample in the dataset that is longer than max_seq_len will be truncated in `truncate()`. The tokenizer’s EOS ids are ensured to be the last token, except in `TextCompletionDataset`.\n",
    "\n",
    "Generally, you want the max sequence length returned in each data sample to match the context window size of your model. You can also decrease this value to reduce memory usage depending on your hardware constraints.\n",
    "\n",
    "```python\n",
    "from torchtune.datasets import alpaca_dataset\n",
    "\n",
    "# Load in tokenizer\n",
    "tokenizer = ...\n",
    "dataset = alpaca_dataset(\n",
    "    tokenizer=tokenizer,\n",
    "    max_seq_len=4096,\n",
    ")\n",
    "```\n",
    "\n",
    "```yaml\n",
    "# YAML config\n",
    "dataset:\n",
    "  _component_: torchtune.datasets.alpaca_dataset\n",
    "  max_seq_len: 4096\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tune run full_finetune_single_device --config llama3/8B_full_single_device \\\n",
    "dataset.max_seq_len=4096"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample packing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use sample packing with any of the single dataset builders by passing in packed=True. This requires some pre-processing of the dataset which may slow down time-to-first-batch, but can introduce significant training speedups depending on the dataset.\n",
    "from torchtune.datasets import alpaca_dataset, PackedDataset\n",
    "\n",
    "```python\n",
    "# Load in tokenizer\n",
    "tokenizer = ...\n",
    "dataset = alpaca_dataset(\n",
    "    tokenizer=tokenizer,\n",
    "    packed=True,\n",
    ")\n",
    "print(isinstance(dataset, PackedDataset))  # True\n",
    "```\n",
    "\n",
    "```yaml\n",
    "# YAML config\n",
    "dataset:\n",
    "  _component_: torchtune.datasets.alpaca_dataset\n",
    "  packed: True\n",
    "```\n",
    "\n",
    "```python\n",
    "!tune run full_finetune_single_device --config llama3/8B_full_single_device \\\n",
    "dataset.packed=True\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
